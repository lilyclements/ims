<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="sec-model-slr" xmlns:xi="http://www.w3.org/2001/XInclude">

<title>Linear regression with a single predictor</title>

<introduction>
  <p>Linear regression is a very powerful statistical technique.</p>
  <p>Many people have some familiarity with regression models just from reading the news, where straight lines are overlaid on scatterplots.</p>
  <p>Linear models can be used for prediction or to describe the relationship between two numerical variables, assuming there is a linear relationship between them.</p>
</introduction>

<section xml:id="sec-fit-line-res-cor">
  <title>Fitting a line, residuals, and correlation</title>

<p>When considering linear regression, it's helpful to think deeply about the line fitting process.</p>

<p>In this section, we define the form of a linear model, explore criteria for what makes a good fit, and introduce a new statistic called <em>correlation</em>.</p>

<subsection xml:id="fitting-a-line-to-data">
  <title>Fitting a line to data</title>

<p><xref ref="fig-perfLinearModel" /> shows two variables whose relationship can be modeled perfectly with a straight line.</p>

<p>The equation for the line is <m>y = 5 + 64.96 x.</m> Consider what a perfect linear relationship means: we know the exact value of <m>y</m> just by knowing the value of <m>x.</m> A perfect linear relationship is unrealistic in almost any natural process.</p>

<p>For example, if we took family income <m>(x),</m> this value would provide some useful information about how much financial support a college may offer a prospective student <m>(y.)</m></p>

<p>However, the prediction would be far from perfect, since other factors play a role in financial support beyond a family's finances.</p>

<figure xml:id="fig-perfLinearModel">
  <caption>Requests from twelve separate buyers were simultaneously placed with a trading company to purchase Target Corporation stock (ticker TGT, December 28th, 2018), and the total cost of the shares was reported. Because the cost is computed using a linear formula, the linear fit is perfect.</caption>
  <image source="images/fig-perfLinearModel-1.png" width="70%" />
</figure>

<p>Linear regression is the statistical method for fitting a line to data where the relationship between two variables, <m>x</m> and <m>y,</m> can be modeled by a straight line with some error:</p>

<me>y = b_0 + b_1 \ x + e</me>

<p>The values <m>b_0</m> and <m>b_1</m> represent the model's intercept and slope, respectively, and the error is represented by <m>e.</m></p>

<p>These values are calculated based on the data, i.e., they are sample statistics.</p>

<p>If the observed data is a random sample from a target population that we are interested in making inferences about, these values are considered to be point estimates for the population parameters <m>\beta_0</m> and <m>\beta_1.</m></p>

<p>We will discuss how to make inferences about parameters of a linear model based on sample statistics in <xref ref="sec-inf-model-slr" />.</p>

<p>When we use <m>x</m> to predict <m>y,</m> we usually call <m>x</m> the <alert>predictor</alert>\index{variable!predictor}\index{predictor variable} variable and we call <m>y</m> the <alert>outcome</alert>\index{variable!outcome}\index{outcome variable}.</p>

<p>We also often drop the <m>e</m> term when writing down the model since our main focus is often on the prediction of the average outcome.</p>

<p>It is rare for all of the data to fall perfectly on a straight line.</p>

<p>Instead, it's more common for data to appear as a <em>cloud of points</em>, such as those examples shown in <xref ref="fig-imperfLinearModel" />.</p>

<p>In each case, the data fall around a straight line, even if none of the observations fall exactly on the line.</p>

<p>The first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between <m>x</m> and <m>y.</m> The second plot shows an upward trend that, while evident, is not as strong as the first.</p>

<p>The last plot shows a very weak downward trend in the data, so slight we can hardly notice it.</p>

<p>In each of these examples, we will have some uncertainty regarding our estimates of the model parameters, <m>\beta_0</m> and <m>\beta_1.</m> For instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less?</p>

<p>As we move forward in this chapter, we will learn about criteria for line-fitting, and we will also learn about the uncertainty associated with estimates of model parameters.</p>

<figure xml:id="fig-imperfLinearModel">
  <caption>Three datasets where a linear model may be useful even though the data do not all fall exactly on the line.</caption>
  <image source="images/fig-imperfLinearModel-1.png" width="70%" />
</figure>

<p>There are also cases where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful.</p>

<p>One such case is shown in <xref ref="fig-notGoodAtAllForALinearModel" /> where there is a very clear relationship between the variables even though the trend is not linear.</p>

<p>We discuss nonlinear trends in this chapter and the next, but details of fitting nonlinear models are saved for a later course.</p>

<figure xml:id="fig-notGoodAtAllForALinearModel">
  <caption>The best fitting line for these data is flat, which is not a useful way to describe the non-linear relationship. These data are from a physics experiment.</caption>
  <image source="images/fig-notGoodAtAllForALinearModel-1.png" width="70%" />
</figure>

</subsection>

<subsection xml:id="using-linear-regression-to-predict-possum-head-lengths">
  <title>Using linear regression to predict possum head lengths</title>

<p>Brushtail possums are marsupials that live in Australia, and a photo of one is shown in <xref ref="fig-brushtail-possum" />.</p>

<p>Researchers captured 104 of these animals and took body measurements before releasing the animals back into the wild.</p>

<p>We consider two of these measurements: the total length of each possum, from head to tail, and the length of each possum's head.</p>

<figure xml:id="fig-brushtail-possum">
  <caption>The common brushtail possum of Australia. Photo by Greg Schecter,</caption>
  <image source="images/fig-brushtail-possum-1.png" width="70%" />
</figure>

<note>
  <title>Data</title>
  <p>The <url href="http://openintrostat.github.io/openintro/reference/possum.html"><c>possum</c></url> data can be found in the <url href="http://openintrostat.github.io/openintro"><alert>openintro</alert></url> R package.</p>
</note>

<p><xref ref="fig-scattHeadLTotalL" /> shows a scatterplot for the head length (mm) and total length (cm) of the possums.</p>

<p>Each point represents a single possum from the data.</p>

<p>The head and total length variables are associated: possums with an above average total length also tend to have above average head lengths.</p>

<p>While the relationship is not perfectly linear, it could be helpful to partially explain the connection between these variables with a straight line.</p>

<figure xml:id="fig-scattHeadLTotalL">
  <caption>A scatterplot showing head length against total length for 104 brushtail possums. A point representing a possum with head length 86.7 mm and total length 84 cm is highlighted.</caption>
  <image source="images/fig-scattHeadLTotalL-1.png" width="70%" />
</figure>

<p>We want to describe the relationship between head and total length of possums with a line.</p>

<p>In this example, we will use the total length as the predictor variable, <m>x,</m> to predict a possum's head length, <m>y.</m> We could fit the linear relationship by eye, as in <xref ref="fig-scattHeadLTotalLLine" />.</p>

<figure xml:id="fig-scattHeadLTotalLLine">
  <caption>A reasonable linear model was fit to represent the relationship between head length and total length.</caption>
  <image source="images/fig-scattHeadLTotalLLine-1.png" width="70%" />
</figure>

<p>The equation for this line is</p>

<me>\hat{y} = 41 + 0.59x</me>

<p>A "hat" on <m>y</m> is used to signify that this is an estimate.</p>

<p>We can use this line to discuss properties of possums.</p>

<p>For instance, the equation predicts a possum with a total length of 80 cm will have a head length of</p>

<me>\hat{y} = 41 + 0.59 \times 80 = 88.2</me>

<p>The estimate may be viewed as an average: the equation predicts that possums with a total length of 80 cm will have an average head length of 88.2 mm.</p>

<p>Absent further information about an 80 cm possum, the prediction for head length that uses the average is a reasonable estimate.</p>

<p>There may be other variables that could help us predict the head length of a possum besides its length.</p>

<p>Perhaps the relationship would be a little different for male possums than female possums, or perhaps it would differ for possums from one region of Australia versus another region.</p>

<p><xref ref="fig-scattHeadLTotalL-sex-age-1" /> shows the relationship between total length and head length of brushtail possums, taking into consideration their sex.</p>

<p>Male possums (represented by blue triangles) seem to be larger in terms of total length and head length than female possums (represented by red circles).</p>

<p><xref ref="fig-scattHeadLTotalL-sex-age-2" /> shows the same relationship, taking into consideration their age.</p>

<p>It's harder to tell if age changes the relationship between total length and head length for these possums.</p>

<figure xml:id="fig-scattHeadLTotalL-sex-age">
  <caption>Relationship between total length and head length of brushtail possums, taking into consideration their sex or age.</caption>
  <image source="images/fig-scattHeadLTotalL-sex-age-1.png" width="70%" />
</figure>

<p>In <xref ref="sec-model-mlr" />, we'll learn about how we can include more than one predictor in our model.</p>

<p>Before we get there, we first need to better understand how to best build a linear model with one predictor.</p>

</subsection>

<subsection xml:id="sec-resids">
  <title>Residuals</title>

<p><alert>Residuals</alert>\index{residuals} are the leftover variation in the data after accounting for the model fit:</p>

<me>\text{Data} = \text{Fit} + \text{Residual}</me>

<p>Each observation will have a residual, and three of the residuals for the linear model we fit for the possum data are shown in <xref ref="fig-scattHeadLTotalLLine-highlighted" />.</p>

<p>If an observation is above the regression line, then its residual, the vertical distance from the observation to the line, is positive.</p>

<p>Observations below the line have negative residuals.</p>

<p>One goal in picking the right linear model is for residuals to be as small as possible.</p>

<p><xref ref="fig-scattHeadLTotalLLine-highlighted" /> is almost a replica of <xref ref="fig-scattHeadLTotalLLine" />, with three points from the data highlighted.</p>

<p>The observation marked by a red circle has a small, negative residual of about -1; the observation marked by a gray diamond has a large positive residual of about +7; and the observation marked by a pink triangle has a moderate negative residual of about -4.</p>

<p>The size of a residual is usually discussed in terms of its absolute value.</p>

<p>For example, the residual for the observation marked by a pink triangle is larger than that of the observation marked by a red circle because <m>|-4|</m> is larger than <m>|-1|.</m></p>

<figure xml:id="fig-scattHeadLTotalLLine-highlighted">
  <caption>A reasonable linear model was fit to represent the relationship between head length and total length, with three points highlighted.</caption>
  <image source="images/fig-scattHeadLTotalLLine-highlighted-1.png" width="70%" />
</figure>

<assemblage>
  <p><alert>Residual: Difference between observed and expected.</alert></p>
  <p>The residual of the <m>i^{th}</m> observation <m>(x_i, y_i)</m> is the difference of the observed outcome <m>(y_i)</m> and the outcome we would predict based on the model fit <m>(\hat{y}_i):</m></p>
  <me>e_i = y_i - \hat{y}_i</me>
  <p>We typically identify <m>\hat{y}_i</m> by plugging <m>x_i</m> into the model.</p>
</assemblage>

<example>
  <statement>
    <p>The linear fit shown in <xref ref="fig-scattHeadLTotalLLine-highlighted" /> is given as <m>\hat{y} = 41 + 0.59x.</m> Based on this line, compute the residual of the observation <m>(76.0, 85.1).</m> This observation is marked by a red circle in <xref ref="fig-scattHeadLTotalLLine-highlighted" />.</p>
    <p>Check it against the earlier visual estimate, -1.</p>
  </statement>
  <solution>
    <p>We first compute the predicted value of the observation marked by a red circle based on the model: <m>\hat{y} = 41+0.59x = 41+0.59\times 76.0 = 85.84</m>.</p>
    <p>Next we compute the difference of the actual head length and the predicted head length: <m>e = y - \hat{y} = 85.1 -  85.84 = -0.74</m>.</p>
    <p>The model's error is <m>e = -0.74</m> mm, which is very close to the visual estimate of -1 mm.</p>
    <p>The negative residual indicates that the linear model overpredicted head length for this possum.</p>
  </solution>
</example>

<exercise>
  <statement>
    <p>If a model underestimates an observation, will the residual be positive or negative?</p>
    <p>What about if it overestimates the observation?</p>
  </statement>
  <solution>
    <p>If a model underestimates an observation, then the model estimate is below the actual.</p>
    <p>The residual, which is the actual observation value minus the model estimate, must then be positive.</p>
    <p>The opposite is true when the model overestimates the observation: the residual is negative.</p>
  </solution>
</exercise>

<exercise>
  <statement>
    <p>Compute the residuals for the observation marked by a blue diamond, <m>(85.0, 98.6),</m> and the observation marked by a pink triangle, <m>(95.5, 94.0),</m> in the figure using the linear relationship <m>\hat{y} = 41 + 0.59x.</m></p>
  </statement>
  <solution>
    <p>Gray diamond: <m>\hat{y} = 41+0.59x = 41+0.59\times 85.0 = 91.15 \rightarrow e = y - \hat{y} = 98.6-91.15=7.45.</m> This is close to the earlier estimate of 7.</p>
    <p>pink triangle: <m>\hat{y} = 41+0.59x = 97.3 \rightarrow e = -3.3.</m> This is also close to the estimate of -4.</p>
  </solution>
</exercise>

<p>Residuals are helpful in evaluating how well a linear model fits a dataset.</p>

<p>We often display them in a scatterplot such as the one shown in <xref ref="fig-scattHeadLTotalLResidualPlot" /> for the regression line in <xref ref="fig-scattHeadLTotalLLine-highlighted" />.</p>

<p>The residuals are plotted with their predicted outcome variable value as the horizontal coordinate, and the vertical coordinate as the residual.</p>

<p>For instance, the point <m>(85.0, 98.6)</m> (marked by the blue diamond) had a predicted value of 91.4 mm and had a residual of 7.45 mm, so in the residual plot it is placed at <m>(91.4, 7.45).</m> Creating a residual plot is sort of like tipping the scatterplot over so the regression line is horizontal, as indicated by the dashed line.</p>

<figure xml:id="fig-scattHeadLTotalLResidualPlot">
  <caption>Residual plot for the model predicting head length from total length for brushtail possums.</caption>
  <image source="images/fig-scattHeadLTotalLResidualPlot-1.png" width="70%" />
</figure>

<example>
  <statement>
    <p>One purpose of residual plots is to identify characteristics or patterns still apparent in the data after fitting a model.</p>
    <p>The figure below shows three scatterplots with linear models in the first row and residual plots in the second row.</p>
    <p>Can you identify any patterns in the residuals?</p>
    </statement>
  <solution>
    <p>Dataset 1: the residuals show no obvious patterns.</p>
    <p>The residuals are scattered randomly around 0, represented by the dashed line.</p>
    <p>Dataset 2: The second dataset shows a pattern in the residuals.</p>
    <p>There is some curvature in the scatterplot, which is more obvious in the residual plot.</p>
    <p>We should not use a straight line to model these data.</p>
    <p>Instead, a more advanced technique should be used to model the curved relationship, such as the variable transformations discussed in <xref ref="sec-transforming-data" />.</p>
    <p>Dataset 3: The last plot shows very little upwards trend, and the residuals also show no obvious patterns.</p>
    <p>It is reasonable to try to fit a linear model to the data.</p>
    <p>However, it is unclear whether there is evidence that the slope parameter is different from zero.</p>
    <p>The point estimate of the slope parameter is not zero, but we might wonder if this could just be due to chance.</p>
    <p>We will address this scenario in <xref ref="sec-inf-model-slr" />.</p>
  </solution>
</example>

</subsection>

<subsection xml:id="describing-linear-relationships-with-correlation">
  <title>Describing linear relationships with correlation</title>

<p>We've seen plots with strong linear relationships and others with very weak linear relationships.</p>

<p>It would be useful if we could quantify the strength of these linear relationships with a statistic.</p>

<assemblage>
  <p><alert>Correlation: strength of a linear relationship.</alert></p>
  <p><alert>Correlation</alert>\index{correlation} which always takes values between -1 and 1, describes the strength and direction of the linear relationship between two variables.</p>
  <p>We denote the correlation by <m>r.</m></p>
  <p>The correlation value has no units and will not be affected by a linear change in the units (e.g., going from inches to centimeters).</p>
</assemblage>

<p>We can compute the correlation using a formula, just as we did with the sample mean and standard deviation.</p>

<p>The formula for correlation, however, is rather complex[^07-model-slr-3], and like with other statistics, we generally perform the calculations on a computer or calculator.</p>

<me>r = \frac{1}{n-1} \sum_{i=1}^{n} \frac{x_i-\bar{x}}{s_x}\frac{y_i-\bar{y}}{s_y}</me>

<p>where <m>\bar{x},</m> <m>\bar{y},</m> <m>s_x,</m> and <m>s_y</m> are the sample means and standard deviations for each variable.</p>

<p><xref ref="fig-posNegCorPlots" /> shows eight plots and their corresponding correlations.</p>

<p>Only when the relationship is perfectly linear is the correlation either -1 or +1.</p>

<p>If the relationship is strong and positive, the correlation will be near +1.</p>

<p>If it is strong and negative, it will be near -1.</p>

<p>If there is no apparent linear relationship between the variables, then the correlation will be near zero.</p>

<figure xml:id="fig-posNegCorPlots">
  <caption>Sample scatterplots and their correlations. The first row shows variables with a positive relationship, represented by the trend up and to the right. The second row shows variables with a negative trend, where a large value in one variable is associated with a lower value in the other.</caption>
  <image source="images/fig-posNegCorPlots-1.png" width="70%" />
</figure>

<p>The correlation is intended to quantify the strength of a linear trend.</p>

<p>Nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship; see three such examples in <xref ref="fig-corForNonLinearPlots" />.</p>

<figure xml:id="fig-corForNonLinearPlots">
  <caption>Sample scatterplots and their correlations. In each case, there is a strong relationship between the variables. However, because the relationship is not linear, the correlation is relatively weak.</caption>
  <image source="images/fig-corForNonLinearPlots-1.png" width="70%" />
</figure>

<exercise>
  <statement>
    <p>No straight line is a good fit for any of the datasets represented in <xref ref="fig-corForNonLinearPlots" />.</p>
    <p>Try drawing nonlinear curves on each plot.</p>
    <p>Once you create a curve for each, describe what is important in your fit.</p>
  </statement>
  <solution>
    <p>We'll leave it to you to draw the lines.</p>
    <p>In general, the lines you draw should be close to most points and reflect overall trends in the data.</p>
  </solution>
</exercise>

<example>
  <statement>
    <p>The plot below displays the relationships between various crop yields in countries.</p>
    <p>In the plots, each point represents a different country.</p>
    <p>The x and y variables represent the proportion of total yield in the last 50 years which is due to that crop type.</p>
    <p>If a country did not produce a particular crop, it has been removed from the plot (so different plots may have different numbers of dots, each corresponding to one country).</p>
    <p>Order the six scatterplots from strongest negative to strongest positive linear relationship.</p>
    </statement>
  <solution>
    <p>The order of most negative correlation to most positive correlation is:</p>
    <p>\vspace{-2mm}</p>
    <p>$$</p>
    <p>A \rightarrow D \rightarrow B \rightarrow C \rightarrow E \rightarrow F</p>
    <p>$$</p>
    <p>-   Plot A - bananas vs. potatoes: <c>r round(cor(crops_country<m>prop_potatoes, crops_country</m>prop_bananas, use = "pairwise.complete.obs"), digits = 2)</c></p>
    <p>-   Plot B - cassava vs. soybeans: <c>r round(cor(crops_country<m>prop_soybeans, crops_country</m>prop_cassava, use = "pairwise.complete.obs"), digits = 2)</c></p>
    <p>-   Plot C - cassava vs. maize: <c>r round(cor(crops_country<m>prop_maize, crops_country</m>prop_cassava, use = "pairwise.complete.obs"), digits = 2)</c></p>
    <p>-   Plot D - cocoa vs. bananas: <c>r round(cor(crops_country<m>prop_cocoa, crops_country</m>prop_bananas, use = "pairwise.complete.obs"), digits = 2)</c></p>
    <p>-   Plot E - peas vs. barley: <c>r round(cor(crops_country<m>prop_peas, crops_country</m>prop_barley, use = "pairwise.complete.obs"), digits = 2)</c></p>
    <p>-   Plot F - wheat vs. barley: <c>r round(cor(crops_country<m>prop_wheat, crops_country</m>prop_barley, use = "pairwise.complete.obs"), digits = 2)</c></p>
  </solution>
</example>

<p>One important aspect of the correlation is that it's <em>unitless</em>.</p>

<p>That is, unlike a measurement of the slope of a line (see the next section) which provides an increase in the y-coordinate for a one unit increase in the x-coordinate (in units of the x and y variable), there are no units associated with the correlation of x and y.</p>

<p><xref ref="fig-bdims-units" /> shows the relationship between weights and heights of 507 physically active individuals.</p>

<p>In <xref ref="fig-bdims-units-1" />, weight is measured in kilograms (kg) and height in centimeters (cm).</p>

<p>In <xref ref="fig-bdims-units-2" />, weight has been converted to pounds (lbs) and height to inches (in).</p>

<p>The correlation coefficient (<m>r = 0.72</m>) is also noted on both plots.</p>

<p>We can see that the shape of the relationship has not changed, and neither has the correlation coefficient.</p>

<p>The only visual change to the plot is the axis <em>labeling</em> of the points.</p>

<figure xml:id="fig-bdims-units">
  <caption>Two scatterplots, both displaying the relationship between weights and heights of 507 physically healthy adults and the correlation coefficient, <m>r = 0.72</m>.</caption>
  <image source="images/fig-bdims-units-1.png" width="70%" />
</figure>

</subsection>

</section>

<section xml:id="sec-least-squares-regression">
  <title>Least squares regression</title>

<p>Fitting linear models by eye is open to criticism since it is based on an individual's preference.</p>

<p>In this section, we use <em>least squares regression</em> as a more rigorous approach to fitting a line to a scatterplot.</p>

<subsection xml:id="gift-aid-for-first-year-at-elmhurst-college">
  <title>Gift aid for first-year at Elmhurst College</title>

<p>This section considers a dataset on family income and gift aid data from a random sample of fifty students in the first-year class of Elmhurst College in Illinois.</p>

<p>Gift aid is financial aid that does not need to be paid back, as opposed to a loan.</p>

<p>A scatterplot of these data is shown in <xref ref="fig-elmhurstScatterWLine" /> along with a linear fit.</p>

<p>The line follows a negative trend in the data; students who have higher family incomes tended to have lower gift aid from the university.</p>

<exercise>
  <statement>
    <p>Is the correlation positive or negative in <xref ref="fig-elmhurstScatterWLine" />?</p>
  </statement>
  <solution>
    <p>Larger family incomes are associated with lower amounts of aid, so the correlation will be negative.</p>
    <p>Using a computer, the correlation can be computed: -0.499.</p>
  </solution>
</exercise>

<figure xml:id="fig-elmhurstScatterWLine">
  <caption>Gift aid and family income for a random sample of 50 first-year students from Elmhurst College.</caption>
  <image source="images/fig-elmhurstScatterWLine-1.png" width="70%" />
</figure>

</subsection>

<subsection xml:id="an-objective-measure-for-finding-the-best-line">
  <title>An objective measure for finding the best line</title>

<p>We begin by thinking about what we mean by the "best" line.</p>

<p>Mathematically, we want a line that has small residuals.</p>

<p>But beyond the mathematical reasons, hopefully it also makes sense intuitively that whatever line we fit, the residuals should be small (i.e., the points should be close to the line).</p>

<p>The first option that may come to mind is to minimize the sum of the residual magnitudes:</p>

<me>|e_1| + |e_2| + \dots + |e_n|</me>

<p>which we could accomplish with a computer program.</p>

<p>The resulting dashed line shown in <xref ref="fig-elmhurstScatterW2Lines" /> demonstrates this fit can be quite reasonable.</p>

<figure xml:id="fig-elmhurstScatterW2Lines">
  <caption>Gift aid and family income for a random sample of 50 first-year Elmhurst College students. The dashed line is the line that minimizes the sum of the absolute value of residuals, the solid line is the line that minimizes the sum of squared residuals, i.e., the least squares line.</caption>
  <image source="images/fig-elmhurstScatterW2Lines-1.png" width="70%" />
</figure>

<p>However, a more common practice is to choose the line that minimizes the sum of the squared residuals:</p>

<me>e_{1}^2 + e_{2}^2 + \dots + e_{n}^2</me>

<p>The line that minimizes this least squares criterion is represented as the solid line in <xref ref="fig-elmhurstScatterW2Lines" /> and is commonly called the <alert>least squares line</alert>\index{least squares line}.</p>

<p>The following are three possible reasons to choose the least squares option instead of trying to minimize the sum of residual magnitudes without any squaring:</p>

<ol>
  <li><p>It is the most commonly used method.</p></li>
  <li><p>Computing the least squares line is widely supported in statistical software.</p></li>
  <li><p>In many applications, a residual twice as large as another residual is more than twice as bad. For example, being off by 4 is usually more than twice as bad as being off by 2. Squaring the residuals accounts for this discrepancy.</p></li>
  <li><p>The analyses which link the model to inference about a population are most straightforward when the line is fit through least squares.</p></li>
</ol>

<p>The first two reasons are largely for tradition and convenience; the third and fourth reasons explain why the least squares criterion is typically most helpful when working with real data.[^07-model-slr-6]</p>

</subsection>

<subsection xml:id="finding-and-interpreting-the-least-squares-line">
  <title>Finding and interpreting the least squares line</title>

<p>For the Elmhurst data, we could write the equation of the least squares regression line as</p>

<me>\widehat{\texttt{aid}} = \beta_0 + \beta_{1}\times \texttt{family\_income}</me>

<p>Here the equation is set up to predict gift aid based on a student's family income, which would be useful to students considering Elmhurst.</p>

<p>These two values, <m>\beta_0</m> and <m>\beta_1,</m> are the parameters of the regression line.</p>

<p>The parameters are estimated using the observed data.</p>

<p>In practice, this estimation is done using a computer in the same way that other estimates, like a sample mean, can be estimated using a computer or calculator.</p>

<p>The dataset where these data are stored is called <c>elmhurst</c>.</p>

<p>The first 5 rows of this dataset are given in <xref ref="tbl-elmhurst-data" />.</p>

<figure xml:id="tbl-elmhurst-data">
  <caption>First five rows of the <c>elmhurst</c> dataset.</caption>
  <image source="images/tbl-elmhurst-data-1.png" width="70%" />
</figure>

<p>We can see that family income is recorded in a variable called <c>family_income</c> and gift aid from university is recorded in a variable called <c>gift_aid</c>.</p>

<p>For now, we won't worry about the <c>price_paid</c> variable.</p>

<p>We should also note that these data are from the 2011-2012 academic year, and all monetary amounts are given in \<m>1,000s, i.e., the family income of the first student in the data shown in <xref ref="tbl-elmhurst-data" /> is \</m>92,920 and they received a gift aid of \$21,700.</p>

<p>(The data source states that all numbers have been rounded to the nearest whole dollar.)</p>

<p>Statistical software is usually used to compute the least squares line and the typical output generated as a result of fitting regression models looks like the one shown in <xref ref="tbl-rOutputForIncomeAidLSRLine" />.</p>

<p>For now we will focus on the first column of the output, which lists <m>{b}_0</m> and <m>{b}_1.</m> In <xref ref="sec-inf-model-slr" /> we will dive deeper into the remaining columns which give us information on how accurate and precise these values of intercept and slope that are calculated from a sample of 50 students are in estimating the population parameters of intercept and slope for <em>all</em> students.</p>

<figure xml:id="tbl-rOutputForIncomeAidLSRLine">
  <caption>Summary of least squares fit for the Elmhurst data.</caption>
  <image source="images/tbl-rOutputForIncomeAidLSRLine-1.png" width="70%" />
</figure>

<p>The model output tells us that the intercept is approximately <c>r m_ga_fi_int</c> and the slope on <c>family_income</c> is approximately <c>r m_ga_fi_slope</c>.</p>

<p>But what do these values mean?</p>

<p>Interpreting parameters in a regression model is often one of the most important steps in the analysis.</p>

<example>
  <statement>
    <p>The intercept and slope estimates for the Elmhurst data are <m>b_0</m> = <c>r m_ga_fi_int</c> and <m>b_1</m> = <c>r m_ga_fi_slope</c>.</p>
    <p>What do these numbers really mean?</p>
  </statement>
  <solution>
    <p>Interpreting the slope parameter is helpful in almost any application.</p>
    <p>For each additional \<m>1,000 of family income, we would expect a student to receive a net difference of 1,000 </m>\times<m> (-0.0431) = -\</m>43.10 in aid on average, i.e., \$43.10 <em>less</em>.</p>
    <p>Note that a higher family income corresponds to less aid because the coefficient of family income is negative in the model.</p>
    <p>We must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational.</p>
    <p>That is, increasing a particular student's family income may not cause the student's aid to drop.</p>
    <p>(Although it would be reasonable to contact the college and ask if the relationship is causal, i.e., if Elmhurst College's aid decisions are partially based on students' family income.)</p>
    <p>The estimated intercept <m>b_0</m> = <c>r m_ga_fi_int</c> describes the average aid if a student's family had no income, \$<c>r format(round(m_ga_fi_int*1000,0), scientific = FALSE, big.mark = ",")</c>.</p>
    <p>The meaning of the intercept is relevant to this application since the family income for some students at Elmhurst is \$0.</p>
    <p>In other applications, the intercept may have little or no practical value if there are no observations where <m>x</m> is near zero.</p>
  </solution>
</example>

<assemblage>
  <p><alert>Interpreting parameters estimated by least squares.</alert></p>
  <p>The slope describes the estimated difference in the predicted average outcome of <m>y</m> if the predictor variable <m>x</m> happened to be one unit larger.</p>
  <p>The intercept describes the average outcome of <m>y</m> if <m>x = 0</m> <em>and</em> the linear model is valid all the way to <m>x = 0</m> (values of <m>x = 0</m> are not observed or relevant in many applications).</p>
</assemblage>

<p>If you would like to learn more about using R to fit linear models, see <xref ref="sec-model-tutorials" /> for the interactive R tutorials.</p>

<p>An alternative way of calculating the values of intercept and slope of a least squares line is manual calculations using formulas.</p>

<p>While manual calculations are not commonly used by practicing statisticians and data scientists, it is useful to work through the first time you're learning about the least squares line and modeling in general.</p>

<p>Calculating the values by hand leverages two properties of the least squares line:</p>

<ol>
  <li><p>The slope of the least squares line can be estimated by</p></li>
</ol>

<me>b_1 = \frac{s_y}{s_x} r</me>

<p>where <m>r</m> is the correlation between the two variables, and <m>s_x</m> and <m>s_y</m> are the sample standard deviations of the predictor and outcome, respectively.</p>

<ol>
  <li><p>If <m>\bar{x}</m> is the sample mean of the predictor variable and <m>\bar{y}</m> is the sample mean of the outcome variable, then the point <m>(\bar{x}, \bar{y})</m> falls on the least squares line.</p></li>
</ol>

<p><xref ref="tbl-summaryStatsElmhurstRegr" /> shows the sample means for the family income and gift aid as \<m>101,780 and \</m>19,940, respectively.</p>

<p>We could plot the point <m>(102, 19.9)</m> on <xref ref="fig-elmhurstScatterWLine" /> to verify it falls on the least squares line (the solid line).</p>

<figure xml:id="tbl-summaryStatsElmhurstRegr">
  <caption>Summary statistics for family income and gift aid.</caption>
  <image source="images/tbl-summaryStatsElmhurstRegr-1.png" width="70%" />
</figure>

<p>Next, we find the point estimates <m>b_0</m> and <m>b_1</m> of the parameters <m>\beta_0</m> and <m>\beta_1.</m></p>

<example>
  <statement>
    <p>Using the summary statistics in <xref ref="tbl-summaryStatsElmhurstRegr" />, compute the slope for the regression line of gift aid against family income.</p>
  </statement>
  <solution>
    <p>Compute the slope using the summary statistics from <xref ref="tbl-summaryStatsElmhurstRegr" />:</p>
    <p>$$</p>
    <p>b_1 = \frac{s_y}{s_x} r = \frac{5.46}{63.2}(-0.499) = -0.0431</p>
    <p>$$</p>
  </solution>
</example>

<p>You might recall the form of a line from math class, which we can use to find the model fit, including the estimate of <m>b_0.</m> Given the slope of a line and a point on the line, <m>(x_0, y_0),</m> the equation for the line can be written as</p>

<me>y - y_0 = slope\times (x - x_0)</me>

<assemblage>
  <p><alert>Identifying the least squares line from summary statistics.</alert></p>
  <p>To identify the least squares line from summary statistics:</p>
  <p>-   Estimate the slope parameter, <m>b_1 = (s_y / s_x) r.</m></p>
  <p>-   Note that the point <m>(\bar{x}, \bar{y})</m> is on the least squares line, use <m>x_0 = \bar{x}</m> and <m>y_0 = \bar{y}</m> with the point-slope equation: <m>y - \bar{y} = b_1 (x - \bar{x}).</m></p>
  <p>-   Simplify the equation, we get <m>y = \bar{y} - b_1 \bar{x} + b_1 x,</m> which reveals that <m>b_0 = \bar{y} - b_1 \bar{x}.</m></p>
</assemblage>

<example>
  <statement>
    <p>Using the point (102, 19.9) from the sample means and the slope estimate <m>b_1 = -0.0431,</m> find the least-squares line for predicting aid based on family income.</p>
  </statement>
  <solution>
    <p>Apply the point-slope equation using <m>(102, 19.9)</m> and the slope <m>b_1 = -0.0431</m>:</p>
    <p>\vspace{-5mm}</p>
    <p>$$</p>
    <p>\begin{aligned}</p>
    <p>y - y_0  &= b_1 (x - x_0) \\</p>
    <p>y - 19.9 &= -0.0431 (x - 102)</p>
    <p>\end{aligned}</p>
    <p>$$</p>
    <p>Expanding the right side and then adding 19.9 to each side, the equation simplifies:</p>
    <p>\vspace{-5mm}</p>
    <p>$$</p>
    <p>\widehat{\texttt{aid}} = 24.3 - 0.0431 \times \texttt{family\_income}</p>
    <p>$$</p>
    <p>Here we have replaced <m>y</m> with <m>\widehat{\texttt{aid}}</m> and <m>x</m> with <m>\texttt{family\_income}</m> to put the equation in context.</p>
    <p>The final least squares equation should always include a "hat" on the variable being predicted, whether it is a generic <m>`<c>y"</m> or a named variable like <m></c>`aid"</m>.</p>
  </solution>
</example>

<example>
  <statement>
    <p>Suppose a high school senior is considering Elmhurst College.</p>
    <p>Can they simply use the linear equation that we have estimated to calculate her financial aid from the university?</p>
  </statement>
  <solution>
    <p>She may use it as an estimate, though some qualifiers on this approach are important.</p>
    <p>First, all data come from one first-year class, and the way aid is determined by the university may change from year to year.</p>
    <p>Second, the equation will provide an imperfect estimate.</p>
    <p>While the linear equation is good at modeling the trend in the data, no individual student's aid will be perfectly predicted (as can be seen from the individual data points around the line).</p>
  </solution>
</example>

</subsection>

<subsection xml:id="extrapolation-is-treacherous">
  <title>Extrapolation is treacherous</title>

<p>Linear models can be used to approximate the relationship between two variables.</p>

<p>However, like any model, they have real limitations.</p>

<p>Linear regression is simply a modeling framework.</p>

<p>The truth is almost always much more complex than a simple line.</p>

<p>For example, we do not know how the data outside of our limited window will behave.</p>

<example>
  <statement>
    <p>Use the model <m>\widehat{\texttt{aid}} = 24.3 - 0.0431 \times \texttt{family\_income}</m> to estimate the aid of another first-year student whose family had income of \$1 million.</p>
  </statement>
  <solution>
    <p>We want to calculate the aid for a family with \$1 million income.</p>
    <p>Note that in our model this will be represented as 1,000 since the data are in \$1,000s.</p>
    <p>\vspace{-5mm}</p>
    <p>$$</p>
    <p>24.3 - 0.0431 \times 1000 = -18.8</p>
    <p>$$</p>
    <p>The model predicts this student will have -\$18,800 in aid (!).</p>
    <p>However, Elmhurst College does not offer <em>negative aid</em> where they select some students to pay extra on top of tuition to attend.</p>
  </solution>
</example>

<p>Applying a model estimate to values outside of the realm of the original data is called <alert>extrapolation</alert>\index{extrapolation}.</p>

<p>Generally, a linear model is only an approximation of the real relationship between two variables.</p>

<p>If we extrapolate, we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been analyzed.</p>

</subsection>

<subsection xml:id="sec-r-squared">
  <title>Describing the strength of a fit</title>

<p>We evaluated the strength of the linear relationship between two variables earlier using the correlation, <m>r.</m> However, it is more common to explain the strength of a linear fit using <m>R^2,</m> called <alert>R-squared</alert>\index{R-squared}.</p>

<p>If provided with a linear model, we might like to describe how closely the data cluster around the linear fit.</p>

<p>The <m>R^2</m> of a linear model describes the amount of variation in the outcome variable that is explained by the least squares line.</p>

<p>For example, consider the Elmhurst data, shown in <xref ref="fig-elmhurstScatterWLine" />).</p>

<p>The variance of the outcome variable, aid received, is about <m>s_{aid}^2 \approx 29.8</m> million (calculated from the data, some of which is shown in <xref ref="tbl-elmhurst-data" />).</p>

<p>However, if we apply our least squares line, then this model reduces our uncertainty in predicting aid using a student's family income.</p>

<p>The variability in the residuals describes how much variation remains after using the model: <m>s_{_{RES}}^2 \approx 22.4</m> million.</p>

<p>In short, there was a reduction of</p>

<md>
  <mrow>\frac{s_{aid}^2 - s_{_{RES}}^2}{s_{aid}^2}</mrow>
  <mrow>= \frac{29800 - 22400}{29800}</mrow>
  <mrow>= \frac{7500}{29800}</mrow>
  <mrow>\approx 0.25,</mrow>
</md>

<p>or about 25%, of the outcome variable's variation by using information about family income for predicting aid using a linear model.</p>

<p>It turns out that <m>R^2</m> corresponds exactly to the squared value of the correlation:</p>

<me>r = -0.499 \rightarrow R^2 = 0.25</me>

<exercise>
  <statement>
    <p>If a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the outcome is explained by the predictor?</p>
  </statement>
  <solution>
    <p>About <m>R^2 = (-0.97)^2 = 0.94</m> or 94% of the variation in the outcome variable is explained by the linear model.</p>
  </solution>
</exercise>

<p><m>R^2</m> is also called the <alert>coefficient of determination</alert>\index{coefficient of determination}.</p>

<assemblage>
  <p><alert>Coefficient of determination: proportion of variability in the outcome variable explained by the model.</alert></p>
  <p>Since <m>r</m> is always between -1 and 1, <m>R^2</m> will always be between 0 and 1.</p>
  <p>This statistic is called the <alert>coefficient of determination</alert>, and it measures the proportion of variation in the outcome variable, <m>y,</m> that can be explained by the linear model with predictor <m>x.</m></p>
</assemblage>

<p>More generally, <m>R^2</m> can be calculated as a ratio of a measure of variability around the line divided by a measure of total variability.</p>

<assemblage>
  <p><alert>Sums of squares to measure variability in</alert> <m>y.</m></p>
  <p>We can measure the variability in the <m>y</m> values by how far they tend to fall from their mean, <m>\bar{y}.</m> We define this value as the <alert>total sum of squares</alert>\index{total sum of squares}, calculated using the formula below, where <m>y_i</m> represents each <m>y</m> value in the sample, and <m>\bar{y}</m> represents the mean of the <m>y</m> values in the sample.</p>
  <p>\vspace{-2mm}</p>
  <me>SST = (y_1 - \bar{y})^2 + (y_2 - \bar{y})^2 + \cdots + (y_n - \bar{y})^2.</me>
  <p>Left-over variability in the <m>y</m> values if we know <m>x</m> can be measured by the <alert>sum of squared errors</alert>\index{sum of squared errors}, or sum of squared residuals, calculated using the formula below, where <m>\hat{y}_i</m> represents the predicted value of <m>y_i</m> based on the least squares regression.[^07-model-slr-9],</p>
  <p>\vspace{-2mm}</p>
  <md>
    <mrow>\begin{aligned}</mrow>
    <mrow>SSE &= (y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \cdots + (y_n - \hat{y}_n)^2\\</mrow>
    <mrow>&= e_{1}^2 + e_{2}^2 + \dots + e_{n}^2</mrow>
    <mrow>\end{aligned}</mrow>
  </md>
  <p>The coefficient of determination can then be calculated as</p>
  <me>R^2 = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST}</me>
</assemblage>

<example>
  <statement>
    <p>Among 50 students in the <c>elmhurst</c> dataset, the total variability in gift aid is <m>SST = 1461</m>.[^07-model-slr-10]</p>
    <p>The sum of squared residuals is <m>SSE = 1098.</m> Find <m>R^2.</m></p>
  </statement>
  <solution>
    <p>Since we know <m>SSE</m> and <m>SST,</m> we can calculate <m>R^2</m> as</p>
    <p>$$</p>
    <p>R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{1098}{1461} = 0.25,</p>
    <p>$$</p>
    <p>the same value we found when we squared the correlation: <m>R^2 = (-0.499)^2 = 0.25.</m></p>
  </solution>
</example>

</subsection>

<subsection xml:id="sec-categorical-predictor-two-levels">
  <title>Categorical predictors with two levels</title>

<p>Categorical variables are also useful in predicting outcomes.</p>

<p>Here we consider a categorical predictor with two levels (recall that a <em>level</em> is the same as a <em>category</em>).</p>

<p>We'll consider Ebay auctions for a video game, <em>Mario Kart</em> for the Nintendo Wii, where both the total price of the auction and the condition of the game were recorded.</p>

<p>Here we want to predict total price based on game condition, which takes values <c>used</c> and <c>new</c>.</p>

<note>
  <title>Data</title>
  <p>The <url href="http://openintrostat.github.io/openintro/reference/mariokart.html"><c>mariokart</c></url> data can be found in the <url href="http://openintrostat.github.io/openintro"><alert>openintro</alert></url> R package.</p>
</note>

<p>A plot of the auction data is shown in <xref ref="fig-marioKartNewUsed" />.</p>

<p>Note that the original dataset contains some Mario Kart games being sold at prices above \<m>100 but for this analysis we have limited our focus to the <c>r nrow(mariokart_lt100)</c> Mario Kart games that were sold below \</m>100.</p>

<figure xml:id="fig-marioKartNewUsed">
  <caption>Total auction prices for the video game Mario Kart, divided into used (<m>x = 0</m>) and new (<m>x = 1</m>) condition games. The least squares regression line is also shown.</caption>
  <image source="images/fig-marioKartNewUsed-1.png" width="70%" />
</figure>

<p>To incorporate the game condition variable into a regression equation, we must convert the categories into a numerical form.</p>

<p>We will do so using an <alert>indicator variable</alert>\index{variable!indicator}\index{indicator variable} called <c>condnew</c>, which takes value 1 when the game is new and 0 when the game is used.</p>

<p>Using this indicator variable, the linear model may be written as</p>

<me>\widehat{\texttt{price}} = b_0 + b_1 \times \texttt{condnew}</me>

<p>The parameter estimates are given in <xref ref="tbl-marioKartNewUsedRegrSummary" />.</p>

<figure xml:id="tbl-marioKartNewUsedRegrSummary">
  <caption>Least squares regression summary for the final auction price against the condition of the game.</caption>
  <image source="images/tbl-marioKartNewUsedRegrSummary-1.png" width="70%" />
</figure>

<p>Using values from <xref ref="tbl-marioKartNewUsedRegrSummary" />, the model equation can be summarized as</p>

<me>\widehat{\texttt{price}} = `r m_total_pr_cond_int` + `r m_total_pr_cond_slope` \times \texttt{condnew}</me>

<example>
  <statement>
    <p>Interpret the two parameters estimated in the model for the price of Mario Kart in eBay auctions.</p>
  </statement>
  <solution>
    <p>The intercept is the estimated price when <c>condnew</c> has a value 0, i.e., when the game is in used condition.</p>
    <p>That is, the average selling price of a used version of the game is \$42.9.</p>
    <p>The slope indicates that, on average, new games sell for about \$10.9 more than used games.</p>
  </solution>
</example>

<assemblage>
  <p><alert>Interpreting model estimates for categorical predictors.</alert></p>
  <p>The estimated intercept is the value of the outcome variable for the first category (i.e., the category corresponding to an indicator value of 0).</p>
  <p>The estimated slope is the average change in the outcome variable between the two categories.</p>
</assemblage>

<p>Note that, fundamentally, the intercept and slope interpretations do not change when modeling categorical variables with two levels.</p>

<p>However, when the predictor variable is binary, the coefficient estimates (<m>b_0</m> and <m>b_1</m>) are directly interpretable with respect to the dataset at hand.</p>

<p>We'll elaborate further on modeling categorical predictors in <xref ref="sec-model-mlr" />, where we examine the influence of many predictor variables simultaneously using multiple regression.</p>

</subsection>

</section>

<section xml:id="sec-outliers-in-regression">
  <title>Outliers in linear regression</title>

<p>In this section, we discuss when outliers are important and influential.</p>

<p>Outliers in a regression model with one predictor and one outcome are observations that fall far from the cloud of points.</p>

<p>These points are especially important because they can have a strong influence on the least squares line.</p>

<p>Note that there are times when observations are outlying in the <m>x</m> direction, the <m>y</m> direction, or both.</p>

<p>However, being outlying in a univariate sense (either <m>x</m> or <m>y</m> or both) is not outlying from the bivariate model.</p>

<p>If the points are in-line with the bivariate model, they will not influence the least squares regression line (even if the observations are outlying in the <m>x</m> or <m>y</m> or both directions!).</p>

<example>
  <statement>
    <p>There are three plots shown in <xref ref="fig-outlier-plots-1" /> along with the corresponding least squares line and residual plots.</p>
    <p>For each scatterplot and residual plot pair, identify the outliers and note how they influence the least squares line.</p>
    <p>Recall that an outlier is any point that does not appear to belong with the vast majority of the other points.</p>
  </statement>
  <solution>
    <p>A: There is one outlier far from the other points (in the <m>y</m> direction and it is an outlier of the bivariate model), though it only appears to slightly influence the line.</p>
    <p>B: There is one outlier on the right (in the <m>x</m> and <m>y</m> direction although it is not an outlier of the bivariate model), though it is quite close to the least squares line, which suggests it wasn't very influential.</p>
    <p>C: There is one point far away from the cloud (in the <m>x</m> and <m>y</m> direction and an outlier of the bivariate model), and this outlier appears to pull the least squares line up on the right; examine how the line around the primary cloud does not appear to fit very well.</p>
  </solution>
</example>

<example>
  <statement>
    <p>There are three plots shown in <xref ref="fig-outlier-plots-2" /> along with the least squares line and residual plots.</p>
    <p>As you did in previous exercise, for each scatterplot and residual plot pair, identify the outliers and note how they influence the least squares line.</p>
    <p>Recall that an outlier is any point that does not appear to belong with the vast majority of the other points. A point can be outlying in the <m>x</m> direction, in the <m>y</m> direction, or in relation to the bivariate model.</p>
  </statement>
  <solution>
    <p>D: There is a primary cloud and then a small secondary cloud of four outliers (with respect to both <m>x</m> and the bivariate model).</p>
    <p>The secondary cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere.</p>
    <p>There might be an interesting explanation for the dual clouds, which is something that could be investigated.</p>
    <p>E: There is no obvious trend in the main cloud of points and the outlier on the right (with respect to both <m>x</m> and <m>y</m>) appears to largely (and problematically) control the slope of the least squares line.</p>
    <p>The point creates a bivariate model when seemingly there is none.</p>
    <p>F: There is one outlier far from the cloud (with respect to both <m>x</m> and <m>y</m>).</p>
    <p>However, it falls quite close to the least squares line and does not appear to be very influential (it is not outlying with respect to the bivariate model).</p>
  </solution>
</example>

<figure xml:id="fig-outlier-plots">
  <caption>Plots of six datasets, each with a least squares line and corresponding residual plot. Each dataset has at least one outlier.</caption>
  <image source="images/fig-outlier-plots-1.png" width="70%" />
</figure>

<p>Examine the residual plots in <xref ref="fig-outlier-plots-1" /> and <xref ref="fig-outlier-plots-2" />.</p>

<p>In Plots C, D, and E, you will probably find that there are a few observations which are both away from the remaining points along the x-axis and not in the trajectory of the trend in the rest of the data.</p>

<p>In these cases, the outliers influenced the slope of the least squares lines.</p>

<p>In Plot E, the bulk of the data show no clear trend, but if we fit a line to these data, we impose a trend where there isn't really one.</p>

<p>A good practice for dealing with outlying observations is to produce two analyses: one with and one without the outlying observations. Presenting both analyses to a client and discussing the role of the outlying observations should lead you to a more holistic understanding of the appropriate model for the data.</p>

<assemblage>
  <p><alert>Leverage.</alert></p>
  <p>Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with <alert>high leverage</alert>\index{high leverage} or <alert>leverage points</alert>\index{leverage point}.</p>
</assemblage>

<p>Points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line.</p>

<p>If one of these high leverage points does appear to actually invoke its influence on the slope of the line -- as in Plots C, D, and E of <xref ref="fig-outlier-plots-1" /> and <xref ref="fig-outlier-plots-2" /> -- then we call it an <alert>influential point</alert>\index{influential point}.</p>

<p>Usually we can say a point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line.</p>

<assemblage>
  <p><alert>Types of outliers.</alert>\index{outlier}</p>
  <p>A point (or a group of points) that stands out from the rest of the data is called an outlier.</p>
  <p>Outliers that fall horizontally away from the center of the cloud of points are called leverage points.</p>
  <p>Outliers that influence on the slope of the line are called influential points.</p>
</assemblage>

<p>It is tempting to remove outliers.</p>

<p>Don't do this without a very good reason.</p>

<p>Models that ignore exceptional (and interesting) cases often perform poorly.</p>

<p>For instance, if a financial firm ignored the largest market swings -- the "outliers" -- they would soon go bankrupt by making poorly thought-out investments.</p>

</section>

<section xml:id="sec-chp7-review">
  <title>Chapter review</title>

<subsection xml:id="summary">
  <title>Summary</title>

<p>Throughout this chapter, the nuances of the linear model have been described.</p>

<p>You have learned how to create a linear model with explanatory variables that are numerical (e.g., total possum length) and those that are categorical (e.g., whether a video game was new).</p>

<p>The residuals in a linear model are an important metric used to understand how well a model fits; high leverage points, influential points, and other types of outliers can impact the fit of a model.</p>

<p>Correlation is a measure of the strength and direction of the linear relationship of two variables, without specifying which variable is the explanatory and which is the outcome.</p>

<p>Future chapters will focus on generalizing the linear model from the sample of data to claims about the population of interest.</p>

</subsection>

<subsection xml:id="terms">
  <title>Terms</title>

<p>The terms introduced in this chapter are presented in <xref ref="tbl-terms-chp-07" />.</p>

<p>If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.</p>

<p>You should be able to easily spot them as <alert>bolded text</alert>.</p>

<figure xml:id="tbl-terms-chp-07">
  <caption>Terms introduced in this chapter.</caption>
  <image source="images/tbl-terms-chp-07-1.png" width="70%" />
</figure>

</subsection>

</section>

<section xml:id="sec-chp7-exercises">
  <title>Exercises</title>

<p>Answers to odd-numbered exercises can be found in <xref ref="sec-exercise-solutions-07" />.</p>

<xi:include href="exercises/_07-ex-model-slr.ptx" />

</section>

</chapter>