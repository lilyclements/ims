<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch12-confidence-intervals-bootstrapping" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Confidence intervals with bootstrapping</title>
  
  <introduction>
    <p>
      In this chapter, we expand on the familiar idea of using a sample proportion to estimate a population proportion.
      That is, we create what is called a <alert>confidence interval</alert>, which is a range of plausible values where we may find the true population value.
      The process for creating a confidence interval is based on understanding how a statistic (here the sample proportion) <em>varies</em> around the parameter (here the population proportion) when many different statistics are calculated from many different samples.
    </p>
    
    <p>
      If we could, we would measure the variability of the statistics by repeatedly taking sample data from the population and compute the sample proportion.
      Then we could do it again.
      And again.
      And so on until we have a good sense of the variability of our original estimate.
    </p>
    
    <p>
      When the variability across the samples is large, we would assume that the original statistic is possibly far from the true population parameter of interest (and the interval estimate will be wide).
      When the variability across the samples is small, we expect the sample statistic to be close to the true parameter of interest (and the interval estimate will be narrow).
    </p>
    
    <p>
      The ideal world where sampling data is free or extremely cheap is almost never the case, and taking repeated samples from a population is usually impossible. So, instead of using a "resample from the population" approach, bootstrapping uses a "resample from the sample" approach.
      In this chapter we discuss in detail the bootstrapping process.
    </p>

  <p>
    As seen in <xref ref="ch11-hypothesis-testing-randomization" />, randomization is a statistical technique suitable for evaluating whether a difference in sample proportions is due to chance.
  </p>
  
  <p>
    Randomization tests are best suited for modeling experiments where the treatment (explanatory variable) has been randomly assigned to the observational units and there is an attempt to answer a simple yes/no research question.
  </p>
  
  <p>
    For example, consider the following research questions that can be well assessed with a randomization test:
  </p>
  
  <p>
    <ul>
      <li>Does this vaccine make it less likely that a person will get malaria?</li>
      <li>Does drinking caffeine affect how quickly a person can tap their finger?</li>
      <li>Can we predict whether candidate A will win the upcoming election?</li>
    </ul>
  </p>
  
  <p>
    In this chapter, however, we are instead interested in a different approach to understanding population parameters.
    Instead of testing a claim, the goal now is to estimate the unknown value of a population parameter.
  </p>
  
  <p>
    For example,
  </p>
  
  <p>
    <ul>
      <li>How much less likely am I to get malaria if I get the vaccine?</li>
      <li>How much faster (or slower) can a person tap their finger, on average, if they drink caffeine first?</li>
      <li>What proportion of the vote will go to candidate A?</li>
    </ul>
  </p>
  
  <p>
    Here, we explore the situation where the focus is on a single proportion, and we introduce a new simulation method: <alert>bootstrapping</alert>.
  </p>
  
  <p>
    Bootstrapping is best suited for modeling studies where the data have been generated through random sampling from a population. As with randomization tests, our goal with bootstrapping is to understand variability of a statistic. Unlike randomization tests (which modeled how the statistic would change if the treatment had been allocated differently), the bootstrap will model how a statistic varies from one sample to another taken from the population.
    This will provide information about how different the statistic is from the parameter of interest.
  </p>
  
  <p>
    Quantifying the variability of a statistic from sample to sample is a hard problem. Fortunately, sometimes the mathematical theory for how a statistic varies (across different samples) is well-known; this is the case for the sample proportion as seen in <xref ref="sec-foundations-mathematical" />.
  </p>
  
  <p>
    However, some statistics do not have simple theory for how they vary, and bootstrapping provides a computational approach for providing interval estimates for almost any population parameter.
    In this chapter we will focus on bootstrapping to estimate a single proportion, and we will revisit bootstrapping in <xref ref="ch19-inference-single-mean" /> through <xref ref="ch21-inference-paired-means" />, so you'll get plenty of practice as well as exposure to bootstrapping in many different data settings.
  </p>
  
  <p>
    Our goal with bootstrapping will be to produce an interval estimate (a range of plausible values) for the population parameter.
  </p>
  </introduction>

  <section xml:id="sec-case-study-med-consult">
    <title>Medical consultant case study</title>
    
    <p>
      People providing an organ for donation sometimes seek the help of a special medical consultant.
      These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery.
      Patients might choose a consultant based in part on the historical complication rate of the consultant's clients.
    </p>
    
    <subsection xml:id="sec-observed-data-med-consult">
      <title>Observed data</title>
      
      <p>
        One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated.
        She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!).
      </p>
      
      <example>
        <statement>
          <p>
            We will let <m>p</m> represent the true complication rate for liver donors working with this consultant.
            (The "true" complication rate will be referred to as the <alert>parameter</alert>.) We estimate <m>p</m> using the data, and label the estimate <m>\hat{p}.</m>
          </p>
        </statement>
        <solution>
          <p>
            The sample proportion for the complication rate is 3 complications divided by the 62 surgeries the consultant has worked on: <m>\hat{p} = 3/62 = 0.048.</m>
          </p>
        </solution>
      </example>
      
      <example>
        <statement>
          <p>
            Is it possible to assess the consultant's claim (that the reduction in complications is due to her work) using the data?
          </p>
        </statement>
        <solution>
          <p>
            No.
            The claim is that there is a causal connection, but the data are observational, so we must be on the lookout for confounding variables.
            For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate.
            While it is not possible to assess the causal claim, it is still possible to understand the consultant's true rate of complications.
          </p>
        </solution>
      </example>
      
      <assemblage>
        <title>Parameter</title>
        <p>
          A <alert>parameter</alert> is the "true" value of interest.
        </p>
        
        <p>
          We typically estimate the parameter using a <alert>point estimate</alert> from a sample of data.
          The point estimate is also known as the <alert>statistic</alert>.
        </p>
        
        <p>
          For example, we estimate the probability <m>p</m> of a complication for a client of the medical consultant by examining the past complication rates of her clients:
        </p>
        
        <p>
          <me>\hat{p} = 3 / 62 = 0.048~\text{is used to estimate}~p</me>
        </p>
      </assemblage>
    </subsection>
    
    <subsection xml:id="sec-variability-statistic-med-consult">
      <title>Variability of the statistic</title>
      
      <p>
        In the medical consultant case study, the parameter is <m>p,</m> the true probability of a complication for a client of the medical consultant.
        There is no reason to believe that <m>p</m> is exactly <m>\hat{p} = 3/62,</m> but there is also no reason to believe that <m>p</m> is particularly far from <m>\hat{p} = 3/62.</m> By sampling with replacement from the dataset (a process called <alert>bootstrapping</alert>), the variability of the possible <m>\hat{p}</m> values can be approximated.
      </p>
      
      <p>
        Most of the inferential procedures covered in this text are grounded in quantifying how one dataset would differ from another when they are both taken from the same population.
        It does not make sense to take repeated samples from the same population because if you have the means to take more samples, a larger sample size will benefit you more than separately evaluating two samples of the exact same size.
        Instead, we measure how the samples behave under an estimate of the population.
      </p>
      
      <p>
        <xref ref="fig-boot1" /> shows how the unknown original population can be estimated by using the sample to approximate the proportion of successes and failures (in our case, the proportion of complications and no complications for the medical consultant).
      </p>
      
      <figure xml:id="fig-boot1">
        <caption>The unknown population is estimated using the observed sample data. Note that we can use the sample to create an estimated or bootstrapped population from which to sample. The observed data include three red and four white marbles, so the estimated population contains 3/7 red marbles and 4/7 white marbles.</caption>
        <image source="images/boot1prop1.png" width="50%" />
      </figure>
      
      <p>
        By taking repeated samples from the estimated population, the variability from sample to sample can be observed.
        In <xref ref="fig-boot2" /> the repeated bootstrap samples are obviously different both from each other and from the original population.
        Recall that the bootstrap samples were taken from the same (estimated) population, and so the differences are due entirely to natural variability in the sampling procedure.
      </p>
      
      <figure xml:id="fig-boot2">
        <caption>Bootstrap sampling provides a measure of the sample to sample variability. Note that we are taking samples from the estimated population that was created from the observed data.</caption>
        <image source="images/boot1prop2.png" width="60%" />
      </figure>
      
      <p>
        By summarizing each of the bootstrap samples (here, using the sample proportion), we see, directly, the variability of the sample proportion, <m>\hat{p},</m> from sample to sample.
        The distribution of <m>\hat{p}_{boot}</m> for the example scenario is shown in <xref ref="fig-boot3" />, and the full bootstrap distribution for the medical consultant data is shown in <xref ref="fig-MedConsBSSim" />.
      </p>
      
      <figure xml:id="fig-boot3">
        <caption>The bootstrapped proportion is estimated for each bootstrap sample. The resulting bootstrap distribution (dotplot) provides a measure for how the proportions vary from sample to sample.</caption>
        <image source="images/boot1prop3.png" width="100%" />
      </figure>
      
      <p>
        It turns out that in practice, it is very difficult for computers to work with an infinite population (with the same proportional breakdown as in the sample).
        However, there is a physical and computational method which produces an equivalent bootstrap distribution of the sample proportion in a computationally efficient manner.
      </p>
      
      <p>
        Consider the observed data to be a bag of marbles 3 of which are success (red) and 4 of which are failures (white).
        By drawing the marbles out of the bag with replacement, we depict the exact same sampling <alert>process</alert> as was done with the infinitely large estimated population.
      </p>
      
      <figure xml:id="fig-boot4">
        <caption>Taking repeated resamples from the sample data is the same process as creating an infinitely large estimate of the population. It is computationally more feasible to take resamples directly from the sample. Note that the resampling is now done with replacement (that is, the original sample does not ever change) so that the original sample and estimated hypothetical population are equivalent.</caption>
        <image source="images/boot1prop4.png" width="70%" />
      </figure>
      
      <figure xml:id="fig-boot1prop">
        <caption>A comparison of the process of sampling from the estimated infinite population and resampling with replacement from the original sample. Note that the dotplot of bootstrapped proportions is the same because the process by which the statistics were estimated is equivalent.</caption>
        <image source="images/boot1propboth.png" width="100%" />
      </figure>
      
      <p>
        If we apply the bootstrap sampling process to the medical consultant example, we consider each client to be one of the marbles in the bag.
        There will be 59 white marbles (no complication) and 3 red marbles (complication).
        If we choose 62 marbles out of the bag (one at a time with replacement) and compute the proportion of simulated patients with complications, <m>\hat{p}_{boot},</m> then this "bootstrap" proportion represents a single simulated proportion from the "resample from the sample" approach.
      </p>
      
      <exercise>
        <statement>
          <p>
            In a simulation of 62 patients, about how many would we expect to have had a complication?
          </p>
        </statement>
        <solution>
          <p>
            About 4.8% of the patients (3 on average) in the simulation will have a complication, as this is what was seen in the sample.
            We will, however, see a little variation from one simulation to the next.
          </p>
        </solution>
      </exercise>
      
      <p>
        One simulation isn't enough to get a sense of the variability from one bootstrap proportion to another bootstrap proportion, so we repeat the simulation 10,000 times using a computer.
      </p>
      
      <p>
        <xref ref="fig-MedConsBSSim" /> shows the distribution from the 10,000 bootstrap simulations.
        The bootstrapped proportions vary from about zero to 11.3%.
        The variability in the bootstrapped proportions leads us to believe that the true probability of complication (the parameter, <m>p</m>) is likely to fall somewhere between 0% and 11.3%, as these numbers capture 95% of the bootstrap resampled values.
      </p>
      
      <p>
        The range of values for the true proportion is called a <alert>bootstrap percentile confidence interval</alert>, and we will see it again throughout the next few sections and chapters.
      </p>
      
      <listing xml:id="listing-fig-MedConsBSSim">
        <caption>R code to generate bootstrap distribution for medical consultant data</caption>
        <program language="r">
          <code>
bsprop_med &lt;- tibble(
  bsprop = rbinom(10000, size = 62, prob = (3 / 62)) / 62
)

bsprop_med_summary &lt;- bsprop_med |&gt;
  summarise(
    bsprop_025 = quantile(bsprop, 0.025),
    bsprop_975 = quantile(bsprop, 0.975),
  )

ggplot(bsprop_med, aes(x = bsprop)) +
  geom_histogram(binwidth = 0.0075) +
  gghighlight(bsprop &lt;= bsprop_med_summary$bsprop_025 | bsprop &gt;= bsprop_med_summary$bsprop_975) +
  annotate("segment",
    x = bsprop_med_summary$bsprop_025, y = 0,
    xend = bsprop_med_summary$bsprop_025, yend = 1000,
    linetype = "dashed"
  ) +
  annotate("segment",
    x = bsprop_med_summary$bsprop_975, y = 0,
    xend = bsprop_med_summary$bsprop_975, yend = 1000,
    linetype = "dashed"
  ) +
  annotate("text", x = bsprop_med_summary$bsprop_025, y = 1200, label = "2.5th\npercentile") +
  annotate("text", x = bsprop_med_summary$bsprop_975, y = 1200, label = "97.5th\npercentile") +
  labs(
    x = "Bootstrapped proportion of surgical complications",
    y = "Count",
    title = "10,000 bootstrapped proportions"
  )
          </code>
        </program>
      </listing>
      
      <figure xml:id="fig-MedConsBSSim">
        <caption>The original medical consultant data is bootstrapped 10,000 times. Each simulation creates a sample from the original data where the probability of a complication is <m>\hat{p} = 3/62.</m> The bootstrap 2.5 percentile proportion is 0 and the 97.5 percentile is 0.113. The result is: we are confident that, in the population, the true probability of a complication is between 0% and 11.3%.</caption>
        <image source="images/fig-MedConsBSSim-1.png" width="70%" />
      </figure>
      
      <example>
        <statement>
          <p>
            The original claim was that the consultant's true rate of complication was under the national rate of 10%.
            Does the interval estimate of 0% to 11.3% for the true probability of complication indicate that the surgical consultant has a lower rate of complications than the national average?
            Explain.
          </p>
        </statement>
        <solution>
          <p>
            No.
            Because the interval overlaps 10%, it might be that the consultant's work is associated with a lower risk of complications, or it might be that the consultant's work is associated with a higher risk (i.e., greater than 10%) of complications!
            Additionally, as previously mentioned, because this is an observational study, even if an association can be measured, there is no evidence that the consultant's work is the cause of the complication rate (being higher or lower).
          </p>
        </solution>
      </example>
    </subsection>
  </section>

  <section xml:id="sec-tapperscasestudy">
    <title>Tappers and listeners case study</title>
    
    <p>
      Here's a game you can try with your friends or family: pick a simple, well-known song, tap that tune on your desk, and see if the other person can guess the song.
      In this simple game, you are the tapper, and the other person is the listener.
    </p>
    
    <subsection xml:id="sec-observed-data-tappers">
      <title>Observed data</title>
      
      <p>
        A Stanford University graduate student named Elizabeth Newton conducted an experiment using the tapper-listener game.<fn>This case study is described in Made to Stick by Chip and Dan Heath. Little known fact: the teaching principles behind many components of this textbook are based on this book!</fn>
        In her study, she recruited 120 tappers and 120 listeners into the study.
        About 50% of the tappers expected that the listener would be able to guess the song.
        Newton wondered, is 50% a reasonable expectation?
      </p>
      
      <p>
        In Newton's study, only 3 out of 120 listeners (<m>\hat{p} = 0.025</m>) were able to guess the tune!
        That seems like quite a low number which leads the researcher to ask: what is the true proportion of people who can guess the tune?
      </p>
    </subsection>
    
    <subsection xml:id="sec-variability-statistic-tappers">
      <title>Variability of the statistic</title>
      
      <p>
        To answer the question, we will again use a simulation.
        To simulate 120 games, this time we use a bag of 120 marbles 3 are red (for those who guessed correctly) and 117 are white (for those who could not guess the song).
        Sampling from the bag 120 times (remembering to replace the marble back into the bag each time to keep constant the population proportion of red) produces one bootstrap sample.
      </p>
      
      <p>
        For example, we can start by simulating 5 tapper-listener pairs by sampling 5 marbles from the bag of 3 red and 117 white marbles.
      </p>
      
      <table>
        <tabular>
          <row>
            <cell>W</cell>
            <cell>W</cell>
            <cell>W</cell>
            <cell>R</cell>
            <cell>W</cell>
          </row>
          <row>
            <cell>Wrong</cell>
            <cell>Wrong</cell>
            <cell>Wrong</cell>
            <cell>Correct</cell>
            <cell>Wrong</cell>
          </row>
        </tabular>
      </table>
      
      <p>
        After selecting 120 marbles, we counted 2 red for <m>\hat{p}_{boot1} = 0.0167.</m> As we did with the randomization technique, seeing what would happen with one simulation isn't enough.
        In order to understand how far the observed proportion of 0.025 might be from the true parameter, we should generate more simulations.
        Here we have repeated the entire simulation ten times:
      </p>
      
      <p>
        <me>0.0417 \quad 0.025 \quad 0.025 \quad 0.0083 \quad 0.05 \quad 0.0333 \quad 0.025 \quad 0 \quad 0.0083 \quad 0</me>
      </p>
      
      <p>
        As before, we'll run a total of 10,000 simulations using a computer.
        As seen in <xref ref="fig-tappers-bs-sim" />, the range of 95% of the resampled values of <m>\hat{p}_{boot}</m> is 0.000 to 0.0583.
        That is, we expect that between 0% and 5.83% of people are truly able to guess the tapper's tune.
      </p>
      
      <listing xml:id="listing-fig-tappers-bs-sim">
        <caption>R code to generate bootstrap distribution for tapper-listener data</caption>
        <program language="r">
          <code>
bsprop_tap &lt;- tibble(
  bsprop = rbinom(10000, size = 120, prob = (3 / 120)) / 120
)

bsprop_tap_summary &lt;- bsprop_tap |&gt;
  summarise(
    bsprop_025 = quantile(bsprop, 0.025),
    bsprop_975 = quantile(bsprop, 0.975),
  )

ggplot(bsprop_tap, aes(x = bsprop)) +
  geom_histogram(binwidth = 0.0045) +
  gghighlight(bsprop &lt;= bsprop_tap_summary$bsprop_025 | bsprop &gt;= bsprop_tap_summary$bsprop_975) +
  annotate("segment",
    x = bsprop_tap_summary$bsprop_025, y = 0,
    xend = bsprop_tap_summary$bsprop_025, yend = 1000,
    linetype = "dashed"
  ) +
  annotate("segment",
    x = bsprop_tap_summary$bsprop_975, y = 0,
    xend = bsprop_tap_summary$bsprop_975, yend = 1000,
    linetype = "dashed"
  ) +
  annotate("text", x = bsprop_tap_summary$bsprop_025, y = 1200, label = "2.5th\npercentile") +
  annotate("text", x = bsprop_tap_summary$bsprop_975, y = 1200, label = "97.5th\npercentile") +
  labs(
    x = "Bootstrapped proportion of listeners who guessed correctly",
    y = "Count",
    title = "10,000 bootstrapped proportions"
  )
          </code>
        </program>
      </listing>
      
      <figure xml:id="fig-tappers-bs-sim">
        <caption>The original listener-tapper data is bootstrapped 10,000 times. Each simulation creates a sample where the probability of being correct is <m>\hat{p} = 3/120.</m> The 2.5 percentile proportion is 0 and the 97.5 percentile is 0.0583. The result is that we are confident that, in the population, the true percent of people who can guess correctly is between 0% and 5.83%.</caption>
        <image source="images/fig-tappers-bs-sim-1.png" width="70%" />
      </figure>
      
      <exercise>
        <statement>
          <p>
            Do the data provide convincing evidence against the claim that 50% of listeners can guess the tapper's tune?
          </p>
        </statement>
        <solution>
          <p>
            Because 50% is not in the interval estimate for the true parameter, we can say that there is convincing evidence against the hypothesis that 50% of listeners can guess the tune.
            Moreover, 50% is a substantial distance from the largest resample statistic, suggesting that there is <alert>very</alert> convincing evidence against this hypothesis.
          </p>
        </solution>
      </exercise>
    </subsection>
  </section>

  <section xml:id="sec-ConfidenceIntervals">
    <title>Confidence intervals</title>
    
    <p>
      A point estimate provides a single plausible value for a parameter.
      However, a point estimate is rarely perfect; usually there is some error in the estimate.
      In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible <em>range of values</em> for the parameter.
    </p>
    
    <subsection xml:id="sec-plausible-range">
      <title>Plausible range of values for the population parameter</title>
      
      <p>
        A plausible range of values for the population parameter is called a <alert>confidence interval</alert>.
        Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net.
        We can throw a spear where we saw a fish, but we will probably miss.
        On the other hand, if we toss a net in that area, we have a good chance of catching the fish.
      </p>
      
      <p>
        If we report a point estimate, we probably will not hit the exact population parameter.
        On the other hand, if we report a range of plausible values <mdash /> a confidence interval <mdash /> we have a good shot at capturing the parameter.
      </p>
      
      <exercise>
        <statement>
          <p>
            If we want to be very certain we capture the population parameter, should we use a wider interval (e.g., 99%) or a smaller interval (e.g., 80%)?
          </p>
        </statement>
        <solution>
          <p>
            If we want to be more certain we will capture the fish, we might use a wider net.
            Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.
          </p>
        </solution>
      </exercise>
    </subsection>
    
    <subsection xml:id="sec-bootstrap-ci">
      <title>Bootstrap confidence interval</title>
      
      <p>
        As we saw above, a <alert>bootstrap sample</alert> is a sample of the original sample.
        In the case of the medical complications data, we proceed as follows:
      </p>
      
      <p>
        <ul>
          <li>Randomly sample one observation from the 62 patients (replace the marble back into the bag so as to keep the population constant).</li>
          <li>Randomly sample a second observation from the 62 patients. Because we sample with replacement (i.e., we do not actually remove the marbles from the bag), there is a 1-in-62 chance that the second observation will be the same one sampled in the first step!</li>
          <li>Keep going one sampled observation at a time <ellipsis /></li>
          <li>Randomly sample the 62nd observation from the 62 patients.</li>
        </ul>
      </p>
      
      <p>
        Bootstrap sampling is often called <alert>sampling with replacement</alert>.
      </p>
      
      <p>
        A bootstrap sample behaves similarly to how an actual sample from a population would behave, and we compute the point estimate of interest (here, compute <m>\hat{p}_{boot}</m>).
      </p>
      
      <p>
        Based on theory that is beyond this text, we know that the bootstrap proportions <m>\hat{p}_{boot}</m> vary around <m>\hat{p}</m> similarly to how different sample proportions (i.e., values of <m>\hat{p}</m>) vary around the true parameter <m>p.</m>
        Therefore, an interval estimate for <m>p</m> can be produced using the <m>\hat{p}_{boot}</m> values themselves.
      </p>
      
      <assemblage>
        <title>95% bootstrap percentile confidence interval for a parameter <m>p</m></title>
        <p>
          The 95% bootstrap confidence interval for the parameter <m>p</m> can be obtained directly using the ordered <m>\hat{p}_{boot}</m> values.
        </p>
        
        <p>
          Consider the sorted <m>\hat{p}_{boot}</m> values.
          Call the 2.5% bootstrapped proportion value "lower", and call the 97.5% bootstrapped proportion value "upper".
        </p>
        
        <p>
          The 95% confidence interval is given by: (lower, upper)
        </p>
      </assemblage>
      
      <p>
        In <xref ref="ch16-inference-single-proportion" /> we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval).
      </p>
      
      <p>
        <xref ref="ch16-inference-single-proportion" /> also provides a longer discussion on what "95% confidence" actually means.
      </p>
    </subsection>
  </section>

  <section xml:id="sec-chp12-review">
    <title>Chapter review</title>
    
    <subsection xml:id="sec-summary-ch12">
      <title>Summary</title>
      
      <p>
        <xref ref="fig-bootboth" /> provides a visual summary of creating bootstrap confidence intervals.
      </p>
      
      <figure xml:id="fig-bootboth">
        <caption>We will use sampling with replacement to measure the variability of the statistic of interest (here the proportion). Sampling with replacement is a computational tool which is equivalent to using the sample as a way of estimating an infinitely large population from which to sample.</caption>
        <image source="images/boot1propboth.png" width="100%" />
      </figure>
      
      <p>
        We can summarize the bootstrap process as follows:
      </p>
      
      <p>
        <ul>
          <li><alert>Frame the research question in terms of a parameter to estimate.</alert> Confidence Intervals are appropriate for research questions that aim to estimate a number from the population (called a parameter).</li>
          <li><alert>Collect data with an observational study or experiment.</alert> If a research question can be formed as a query about the parameter, we can collect data to calculate a statistic which is the best guess we have for the value of the parameter. However, we know that the statistic won't be exactly equal to the parameter due to natural variability.</li>
          <li><alert>Model the randomness by using the data values as a proxy for the population.</alert> In order to assess how far the statistic might be from the parameter, we take repeated resamples from the dataset to measure the variability in bootstrapped statistics. The variability of the bootstrapped statistics around the observed statistic (a quantity which can be measured through computational technique) should be approximately the same as the variability of many observed sample statistics around the parameter (a quantity which is very difficult to measure because in real life we only get exactly one sample).</li>
          <li><alert>Create the interval.</alert> After choosing a particular confidence level, use the variability of the bootstrapped statistics to create an interval estimate which will hope to capture the true parameter. While the interval estimate associated with the particular sample at hand may or may not capture the parameter, the researcher knows that over their lifetime, the confidence level will determine the percentage of their research confidence intervals that do capture the true parameter.</li>
          <li><alert>Form a conclusion.</alert> Using the confidence interval from the analysis, report on the interval estimate for the parameter of interest. Also, be sure to write the conclusion in plain language so casual readers can understand the results.</li>
        </ul>
      </p>
      
      <p>
        <xref ref="tbl-chp12-summary" /> is another look at the bootstrap process summary.
      </p>
      
      <table xml:id="tbl-chp12-summary">
        <title>Summary of bootstrapping as an inferential statistical method.</title>
        <tabular>
          <row header="yes">
            <cell>Question</cell>
            <cell>Answer</cell>
          </row>
          <row>
            <cell>What does it do?</cell>
            <cell>Resamples (with replacement) from the observed data to mimic the sampling variability found by collecting data from a population</cell>
          </row>
          <row>
            <cell>What is the random process described?</cell>
            <cell>Random sampling from a population</cell>
          </row>
          <row>
            <cell>What other random processes can be approximated?</cell>
            <cell>Can also be used to describe random allocation in an experiment</cell>
          </row>
          <row>
            <cell>What is it best for?</cell>
            <cell>Confidence intervals (can also be used for bootstrap hypothesis testing for one proportion as well)</cell>
          </row>
          <row>
            <cell>What physical object represents the simulation process?</cell>
            <cell>Pulling marbles from a bag with replacement</cell>
          </row>
        </tabular>
      </table>
    </subsection>
    
    <subsection xml:id="sec-terms-ch12">
      <title>Terms</title>
      
      <p>
        The terms introduced in this chapter are presented below.
        If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
        You should be able to easily spot them as <alert>bolded text</alert>.
      </p>
      
      <p>
        <ul>
          <li>bootstrapping</li>
          <li>parameter</li>
          <li>point estimate</li>
          <li>statistic</li>
          <li>bootstrap sample</li>
          <li>bootstrap percentile confidence interval</li>
          <li>sampling with replacement</li>
          <li>confidence interval</li>
        </ul>
      </p>
    </subsection>
  </section>

  <section xml:id="sec-chp12-exercises">
    <title>Exercises</title>
    
    <p>
      Answers to odd-numbered exercises can be found in <xref ref="sec-exercise-solutions-12" />.
    </p>
    
    <xi:include href="../exercises/_12-ex-foundations-bootstrapping.ptx" />
  </section>
</chapter>
